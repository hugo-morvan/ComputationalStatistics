---
title: "732A90 Lab 2"
author: "Daniele Bozzi & Hugo Morvan"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1: Optimisation of a two-dimensional function

### a)
```{r}
f <- function(p){
  x <- p[1]
  y <- p[2]
  return(-x^2 -x^2*y^2 -2*x*y +2*x +2)
}
f_x <- function(p){
  x <- p[1]
  y <- p[2]
  return(-2*x -2*x*y^2 -2*y +2)
}
f_y <- function(p){
  x <- p[1]
  y <- p[2]
  return(-2*x^2*y-2*x)
}
f_xy <- function(p){
  x <- p[1]
  y <- p[2]
  return(-4*x*y-2)
}
f_xx <- function(p){
  x <- p[1]
  y <- p[2]
  return(-2 -2*y^2)
}
f_yy <- function(p){
  x <- p[1]
  y <- p[2]
  return(-2*x^2)
}
get_gradient <- function(p){
  return(c(f_x(p), f_y(p)))
}
get_hessian <- function(p){
  hess <- matrix(c(f_xx(p), f_xy(p), f_xy(p), f_yy(p)), ncol=2)
  return(hess)
}

x <- seq(-3,3,0.05)
y <- seq(-3,3,0.05)

lx <- length(x)
ly <- length(y)

points1 <- matrix(0,nrow=lx, ncol=ly)

for (i in 1:lx){
  for (q in 1:ly){
    points1[i,q] <- f(c(x[i], y[q]))
  }
}

contour(x,y,points1, nlevels=50)
# unnecessary not so good looking 3d plot
persp(x, y, points1, xlab="x", ylab="y", zlab="z", theta=90, phi=20, zlim=c(0, 5))

```

### b)

```{r}
newton <- function(x, epsilon){
  #x is you starting vector
  dist <- 999
  
  while(dist > epsilon){
    gp <- get_gradient(x)
    gpp <- get_hessian(x)
    x_next <- x - solve(gpp)%*%gp
    dist <- sum((x-x_next)*(x-x_next))
    x <- x_next
  }
  return(x)
}
```

### c)

```{r}
p1 <- c(2,0)
p2 <- c(-1,-2)
p3 <- c(0,1)
p4 <- c(10,-10)
epsilon <- 1E-8
newton(p1, epsilon)
newton(p2, epsilon)
newton(p3, epsilon)
newton(p4, epsilon)

```
We obtain 2 points : (0,1) and (1,-1)

```{r}
print("Gradient and Hessian for the point (0,1)")
get_gradient(c(0,1))
get_hessian(c(0,1))
print("this is a saddle point")
#print("the matrix is negative semi-definite")

print("Gradient and Hessian for the point (1,-1)")
get_gradient(c(1,-1))
get_hessian(c(1,-1))

print("this is a maximum")

```


### d)
The disadvantage of using the steepest method instead of the Newton one is that it
converges slower, but the advantage is that the computation of the Hessian matrix is 
not needed, which can be difficult sometimes to calculate.


## Question 2:

### a)

```{r}

X <- c(0,0,0,0.1,0.1,0.3,0.3,0.9,0.9,0.9)
Y <- c(0,0,1,0,1,1,1,0,1,1)

g <- function(B){
  sum1 <- 0
  for (q in length(X))
    sum1 <- sum1 + (Y[q]*log((1+exp(-B[1]-B[2]*X[q]))^-1)+(1-Y[q])*log(1-(1+exp(-B[1]-B[2]*X[q]))^-1))
  return(sum1)
}

grad <- function(B){
  sum2 <- 0
  for (i in length(X))
    sum2 <- sum2 + (Y[i] -(1/(1+exp(-B[1]-B[2]*X[i]))) * c(1,X[i]))
  return(sum2)
}

steepestasc <- function(x0, eps=1e-5, alpha0=1)
{
  countfun <- 0
  countgrad <- 0
  xt   <- x0
  conv <- 999
  #points(xt[1], xt[2], col=2, pch=4, lwd=3)
  while(conv>eps)
  {
    alpha <- alpha0
    xt1   <- xt
    xt    <- xt1 + alpha*grad(xt1)
    countgrad <- countgrad + 1
    cat(xt, xt1, "\n")
    while (g(xt) < g(xt1))
    {
      cat(xt, xt1, "INSIDE\n")
      countfun <- countfun + 2
      alpha <- alpha/2
      xt    <- xt1 + alpha*grad(xt1)
      countgrad <- countgrad + 1
    }
    #points(xt[1], xt[2], col=2, pch=4, lwd=1)
    conv <- sum((xt-xt1)*(xt-xt1))
  }
  #points(xt[1], xt[2], col=4, pch=4, lwd=3)
  cat(xt, "\n")
  cat("Function calls:", countfun, "\nGradiant calls:", countgrad,"\n")
}

```

### b)

```{r}

steepestasc(c(-0.2, 1))

```


### c)

```{r}

optim(c(-0.2, 1), g, gr = grad, method= "Nelder-Mead")
xt <- c(-0.2,1)

xt1   <- xt
xt    <- xt1 + 1*grad(xt1)

g(xt) < g(xt1)

```


### d)
